{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#First, make sure the notebook is aware of the workshop data sets\n",
        "!git clone https://github.com/icomse/9th_workshop_ml_for_molecules.git\n",
        "import os\n",
        "os.chdir('9th_workshop_ml_for_molecules/data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xs9ebYrN_8YE",
        "outputId": "f34b86fc-5c1d-42aa-f1ad-092a05d86c8d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path '9th_workshop_ml_for_molecules' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pIs8p5r6fB5",
        "outputId": "abb05f35-f3b7-4072-bcdd-ad8c793c2854"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m√ó\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m√ó\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m√ó\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m√ó\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8s96icrJ6Txn",
        "outputId": "12be7bc6-ab74-41f0-a7aa-a6f5c9362035"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ú®üç∞‚ú® Everything looks OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mamba install -c conda-forge rdkit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dA3MYXa6bOU",
        "outputId": "ffa817c3-bb52-4aac-f27f-e2a1e579da15"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Looking for: ['rdkit']\n",
            "\n",
            "\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
            "conda-forge/linux-64  ‚£æ  \n",
            "conda-forge/noarch    ‚£æ  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\n",
            "conda-forge/linux-64  ‚£æ  \n",
            "conda-forge/noarch    ‚£æ  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.2s\n",
            "conda-forge/linux-64   4%\n",
            "conda-forge/noarch     1%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.3s\n",
            "conda-forge/linux-64   6%\n",
            "conda-forge/noarch     6%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.4s\n",
            "conda-forge/linux-64   9%\n",
            "conda-forge/noarch    13%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.5s\n",
            "conda-forge/linux-64  13%\n",
            "conda-forge/noarch    18%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.6s\n",
            "conda-forge/linux-64  14%\n",
            "conda-forge/noarch    24%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.7s\n",
            "conda-forge/linux-64  15%\n",
            "conda-forge/noarch    26%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.8s\n",
            "conda-forge/linux-64  17%\n",
            "conda-forge/noarch    28%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.9s\n",
            "conda-forge/linux-64  19%\n",
            "conda-forge/noarch    33%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.0s\n",
            "conda-forge/linux-64  21%\n",
            "conda-forge/noarch    38%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.1s\n",
            "conda-forge/linux-64  22%\n",
            "conda-forge/noarch    41%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.2s\n",
            "conda-forge/linux-64  25%\n",
            "conda-forge/noarch    47%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.3s\n",
            "conda-forge/linux-64  26%\n",
            "conda-forge/noarch    50%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.4s\n",
            "conda-forge/linux-64  29%\n",
            "conda-forge/noarch    53%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.5s\n",
            "conda-forge/linux-64  29%\n",
            "conda-forge/noarch    57%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.6s\n",
            "conda-forge/linux-64  31%\n",
            "conda-forge/noarch    60%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.7s\n",
            "conda-forge/linux-64  32%\n",
            "conda-forge/noarch    62%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.8s\n",
            "conda-forge/linux-64  33%\n",
            "conda-forge/noarch    63%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.9s\n",
            "conda-forge/linux-64  33%\n",
            "conda-forge/noarch    65%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.0s\n",
            "conda-forge/linux-64  34%\n",
            "conda-forge/noarch    67%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.1s\n",
            "conda-forge/linux-64  35%\n",
            "conda-forge/noarch    67%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.2s\n",
            "conda-forge/linux-64  37%\n",
            "conda-forge/noarch    71%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.3s\n",
            "conda-forge/linux-64  37%\n",
            "conda-forge/noarch    74%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.4s\n",
            "conda-forge/linux-64  39%\n",
            "conda-forge/noarch    76%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.5s\n",
            "conda-forge/linux-64  39%\n",
            "conda-forge/noarch    80%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.6s\n",
            "conda-forge/linux-64  41%\n",
            "conda-forge/noarch    81%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.7s\n",
            "conda-forge/linux-64  43%\n",
            "conda-forge/noarch    83%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.8s\n",
            "conda-forge/linux-64  43%\n",
            "conda-forge/noarch    88%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.9s\n",
            "conda-forge/linux-64  46%\n",
            "conda-forge/noarch    90%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.0s\n",
            "conda-forge/linux-64  49%\n",
            "conda-forge/noarch    97%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.1s\n",
            "conda-forge/linux-64  49%\n",
            "conda-forge/noarch   100%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gconda-forge/noarch                                \n",
            "[+] 3.2s\n",
            "conda-forge/linux-64  50%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.3s\n",
            "conda-forge/linux-64  54%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.4s\n",
            "conda-forge/linux-64  57%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.5s\n",
            "conda-forge/linux-64  59%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.6s\n",
            "conda-forge/linux-64  61%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.7s\n",
            "conda-forge/linux-64  63%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.8s\n",
            "conda-forge/linux-64  65%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.9s\n",
            "conda-forge/linux-64  69%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.0s\n",
            "conda-forge/linux-64  73%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.1s\n",
            "conda-forge/linux-64  74%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.2s\n",
            "conda-forge/linux-64  77%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.3s\n",
            "conda-forge/linux-64  79%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.4s\n",
            "conda-forge/linux-64  83%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.5s\n",
            "conda-forge/linux-64  86%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.6s\n",
            "conda-forge/linux-64  92%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.7s\n",
            "conda-forge/linux-64  94%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.8s\n",
            "conda-forge/linux-64  94%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.9s\n",
            "conda-forge/linux-64  94%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.0s\n",
            "conda-forge/linux-64  94%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.1s\n",
            "conda-forge/linux-64  94%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.2s\n",
            "conda-forge/linux-64  94%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.3s\n",
            "conda-forge/linux-64  94%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.4s\n",
            "conda-forge/linux-64  94%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.5s\n",
            "conda-forge/linux-64  95%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.6s\n",
            "conda-forge/linux-64  95%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.7s\n",
            "conda-forge/linux-64  98%\u001b[2K\u001b[1A\u001b[2K\u001b[0Gconda-forge/linux-64                              \n",
            "\u001b[?25h\n",
            "Pinned packages:\n",
            "  - python 3.11.*\n",
            "  - python 3.11.*\n",
            "  - python_abi 3.11.* *cp311*\n",
            "  - cuda-version 12.*\n",
            "\n",
            "\n",
            "Transaction\n",
            "\n",
            "  Prefix: /usr/local\n",
            "\n",
            "  All requested packages already installed\n",
            "\n",
            "\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "K_0lvzJpacZF"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "from rdkit import Chem\n",
        "\n",
        "import torch\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some Useful Functions:"
      ],
      "metadata": {
        "id": "es8KhqKUO42b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I usually keep these in a utils.py file and import them into my other files:\n",
        "def smiles2geodata(smi,y, feat_dict):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    smi = smiles string\n",
        "    y = target\n",
        "    feat_dict = dictionary where keys are atoms and values are feature array (constructed using get_atom_features function)\n",
        "\n",
        "    Outputs:\n",
        "    geo_dp = pytorch geometric datapoint object\n",
        "    \"\"\"\n",
        "    mol = Chem.MolFromSmiles(smi)\n",
        "    atomic_nums = [atom.GetAtomicNum() for atom in mol.GetAtoms()]\n",
        "    atom_features = torch.tensor([feat_dict[x] for x in atomic_nums]).float()\n",
        "    edges = get_edge_indices(mol)\n",
        "    geo_dp = Data(x=atom_features, edge_index=edges,y=y)\n",
        "\n",
        "    return geo_dp\n",
        "\n",
        "\n",
        "def get_edge_indices(mol):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    mol = rdkit molecule object\n",
        "\n",
        "    Outputs:\n",
        "    torch tensor containing edge list (for inputting into smiles2geodata)\n",
        "    \"\"\"\n",
        "\n",
        "    edges =[]\n",
        "    for bond in mol.GetBonds():\n",
        "        edges.append((bond.GetBeginAtomIdx(),bond.GetEndAtomIdx()))\n",
        "\n",
        "    edges = [[x[0] for x in edges],[x[1] for x in edges]]\n",
        "\n",
        "    return torch.tensor(edges,dtype=torch.long)\n",
        "\n",
        "\n",
        "def get_atom_features(smi_list):\n",
        "    \"\"\"\n",
        "    One-hot encodes atom types and constructs a dictionary to convert atom types into feature vectors\n",
        "\n",
        "    Inputs:\n",
        "    smi_list = list of all smiles in the dataset\n",
        "\n",
        "    Outputs:\n",
        "    feat_dict = dictionary where keys are atoms and values are feature array\n",
        "    \"\"\"\n",
        "\n",
        "    atom_types = []\n",
        "    for smi in smi_list:\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        atom_types.extend([atom.GetAtomicNum() for atom in mol.GetAtoms()])\n",
        "\n",
        "    atom_set = list(set(atom_types))\n",
        "\n",
        "    enc = OneHotEncoder()\n",
        "    enc.fit(np.array(atom_set).reshape(-1,1))\n",
        "    feat_dict = {x:enc.transform([[x]]).toarray()[0] for x in atom_set}\n",
        "\n",
        "    return feat_dict"
      ],
      "metadata": {
        "id": "95y_ioEAO24g"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a deep learning model in Pytorch requires building multiple objects and routines:\n",
        "\n",
        "1. <u>Dataset</u>- object that contains your training/validation data\n",
        "\n",
        "2. <u>Model</u>- object that contains your model architecture and weights\n",
        "\n",
        "3. <u>Train Routine</u>- function that passes training data through model and updates weights\n",
        "\n",
        "4. <u>Validation Routine</u>- function that passes validation through model and reports performance"
      ],
      "metadata": {
        "id": "wOj87LmFNvsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dataset"
      ],
      "metadata": {
        "id": "BiR1lG_2Nyyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = 'aqsol.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "A8GtZI1oNwGC",
        "outputId": "d6c10e63-42a0-47ac-a538-2e0c4b3ef1ec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'aqsol.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-452103ab3719>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'aqsol.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'aqsol.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import InMemoryDataset\n",
        "from torch_geometric.data import Data"
      ],
      "metadata": {
        "id": "DNYBSTCJQk30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GeoDataset(InMemoryDataset):\n",
        "    def __init__(self, root='./',raw_name=data_path,processed_name='lipo_processed.pt',transform=None, pre_transform=None):\n",
        "\n",
        "        self.filename = os.path.join(root,raw_name)\n",
        "        # self.processed_filename = os.path.join(root,processed_name)\n",
        "\n",
        "        # read a csv from that path:\n",
        "        self.df = pd.read_csv(self.filename)\n",
        "\n",
        "        # assign dataset attribute \"input_vectors\" to be the 2048 bit vector representing each molecule:\n",
        "        self.x = self.df[self.df.columns[0]].values\n",
        "\n",
        "        # assign dataset attribute \"output_targets\" to be the scalar representing binding strength (last column):\n",
        "        self.y = self.df[self.df.columns[-1]].values\n",
        "\n",
        "\n",
        "        super(GeoDataset, self).__init__(root, transform, pre_transform)\n",
        "\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0],weights_only=False)\n",
        "\n",
        "\n",
        "    def processed_file_names(self):\n",
        "        return ['data.pt']\n",
        "\n",
        "    def process(self):\n",
        "\n",
        "        feat_dict = get_atom_features(self.x)\n",
        "\n",
        "        data_list = [smiles2geodata(x,y,feat_dict) for x,y in zip(self.x,self.y)]\n",
        "\n",
        "        data, slices = self.collate(data_list)\n",
        "\n",
        "        torch.save((data, slices), self.processed_paths[0])\n",
        ""
      ],
      "metadata": {
        "id": "X8uxk_RXN2OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Model"
      ],
      "metadata": {
        "id": "UGRl3DCYQ0MW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import aggr"
      ],
      "metadata": {
        "id": "AJRZ4JBKQghV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN_Geo(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_dim_gcn=256, hidden_dim_fcn=256):\n",
        "        super(GCN_Geo, self).__init__()\n",
        "\n",
        "        self.conv1 = GCNConv(12, hidden_dim_gcn)\n",
        "        self.conv2 = GCNConv(hidden_dim_gcn, hidden_dim_gcn)\n",
        "\n",
        "        self.readout = aggr.SumAggregation()\n",
        "\n",
        "        self.linear1 = nn.Linear(hidden_dim_gcn, hidden_dim_fcn)\n",
        "        self.linear2 = nn.Linear(hidden_dim_fcn, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "\n",
        "        # Message passing layers:\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Aggregate to get molecule level features:\n",
        "        x= self.readout(x,data.batch)\n",
        "\n",
        "        # FCNN to predict molecular property:\n",
        "        x = self.linear1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.linear2(x)\n",
        "\n",
        "        return x.view(-1,)"
      ],
      "metadata": {
        "id": "6mQtQ1K0QosU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pulling it all together"
      ],
      "metadata": {
        "id": "4zTbHrdq5Oih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader"
      ],
      "metadata": {
        "id": "_VHifpZW5KkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cpu')\n",
        "model = GCN_Geo().to(device)\n",
        "\n",
        "dataset = GeoDataset()\n",
        "dataloader = DataLoader(dataset,batch_size=32,shuffle=True)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "loss_fn = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "uOQV8tJF5VNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(30):\n",
        "    print('Epoch: {}'.format(epoch))\n",
        "    for batch in dataloader:\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out = model(batch)\n",
        "\n",
        "        loss = loss_fn(out.double(), batch.y.double())\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "h2A8_AR_9mZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, dataloader):\n",
        "\n",
        "    # Set our model to evaluation mode:\n",
        "    model.eval()\n",
        "\n",
        "    X_all = []\n",
        "    y_all = []\n",
        "    pred_all = []\n",
        "\n",
        "    # Remove gradients:\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Looping over the dataloader allows us to pull out or input/output data:\n",
        "        for batch in dataloader:\n",
        "\n",
        "            # Make a prediction:\n",
        "            pred = model(batch)\n",
        "\n",
        "            X_all.append(batch.x)\n",
        "            y_all.append(batch.y)\n",
        "            pred_all.append(pred)\n",
        "\n",
        "    X_all = torch.concat(X_all)\n",
        "    y_all = torch.concat(y_all)\n",
        "    pred_all = torch.concat(pred_all)\n",
        "\n",
        "    return X_all, y_all, pred_all"
      ],
      "metadata": {
        "id": "aj23zG6R-834"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x,y,pred = predict(model,dataloader)"
      ],
      "metadata": {
        "id": "HRl3jB4g_GDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(4,4))\n",
        "plt.scatter(y,pred,alpha=0.4)\n",
        "plt.plot([-1,4],[-1,4],color='k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RyqFIvbX_Owt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(y,pred)"
      ],
      "metadata": {
        "id": "AqrsBoas_Pbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***HACKING:*** We didn't include training/validation splits! Can you go back and including training and validation splits, using the FCNN code from the previous section as a reference?"
      ],
      "metadata": {
        "id": "cnsH4AJ4F312"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JMqheQtoATiL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}